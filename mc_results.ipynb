{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import dask\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import shutil\n",
    "\n",
    "\n",
    "def load_pcd(pcd_path: Path) -> np.ndarray:\n",
    "    return np.asarray(o3d.io.read_point_cloud(str(pcd_path)).points).astype(np.float32)\n",
    "\n",
    "\n",
    "def write_pcd(\n",
    "    xyz: np.ndarray,\n",
    "    path: Path,\n",
    ") -> bool:\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(xyz)\n",
    "    o3d.io.write_point_cloud(str(path), pcd)\n",
    "    return True\n",
    "\n",
    "\n",
    "def lazy_load_pcd_stack(pcd_list):\n",
    "    # load_pcd the first pcd (assume rest are same shape/dtype)\n",
    "    sample = load_pcd(pcd_list[0])\n",
    "\n",
    "    # Build a list of lazy dask arrays\n",
    "    arrays = [\n",
    "        da.from_delayed(\n",
    "            dask.delayed(load_pcd)(path),\n",
    "            dtype=sample.dtype,\n",
    "            shape=sample.shape,\n",
    "        )\n",
    "        for path in pcd_list\n",
    "    ]\n",
    "    # Stack all point coordinates into a 3D dask array\n",
    "    stack = da.stack(arrays, axis=0)\n",
    "    stack = stack.rechunk()\n",
    "    return stack\n",
    "\n",
    "\n",
    "def load_pcd_stack(pcd_list):\n",
    "    arrays = [load_pcd(path) for path in pcd_list]\n",
    "    stack = np.stack(arrays, axis=0)\n",
    "    return stack\n",
    "\n",
    "\n",
    "def generate_random_data(directory, npcd, npoints, noise_std):\n",
    "    directory = Path(directory)\n",
    "    if directory.exists():\n",
    "        shutil.rmtree(directory)\n",
    "    directory.mkdir(parents=True)\n",
    "\n",
    "    def random_points3d_uniform(\n",
    "        npoints=1000,\n",
    "        xlim=(100, 200),\n",
    "        ylim=(100, 200),\n",
    "        zlim=(0, 1),\n",
    "        datatype=np.float32,\n",
    "    ):\n",
    "        rng = np.random.default_rng(seed=0)\n",
    "        x = rng.uniform(xlim[0], xlim[1], npoints)\n",
    "        y = rng.uniform(ylim[0], ylim[1], npoints)\n",
    "        z = rng.uniform(zlim[0], zlim[1], npoints)\n",
    "        return np.stack([x, y, z], axis=1).astype(datatype)\n",
    "\n",
    "    def generate_simulated_pcd(pcd_path, ref_pcd_path, noise_std):\n",
    "        rng = np.random.default_rng()\n",
    "        xyz = load_pcd(ref_pcd_path)\n",
    "        # add noise\n",
    "        # x = xyz[:, 0]\n",
    "        # y = xyz[:, 1]\n",
    "        # z = xyz[:, 2]\n",
    "        # x += rng.normal(0, noise_std, x.shape)\n",
    "        # y += rng.normal(0, noise_std, y.shape)\n",
    "        # z += rng.normal(0, noise_std, z.shape)\n",
    "        # xyz = np.stack([x, y, z], axis=1)\n",
    "        xyz += rng.normal(0, noise_std, xyz.shape)\n",
    "        return write_pcd(xyz, pcd_path)\n",
    "\n",
    "    # generate random reference pcd\n",
    "    ref_pcd_path = directory / \"sparse_pts_reference.ply\"\n",
    "    xyz = random_points3d_uniform(npoints)\n",
    "    write_pcd(xyz, ref_pcd_path)\n",
    "\n",
    "    # generate random pcds starting from reference and adding noise\n",
    "    output_directory = directory / \"Monte_Carlo_output\"\n",
    "    output_directory.mkdir()\n",
    "\n",
    "    if npcd > 1000:\n",
    "        delayed_tasks = []\n",
    "        for i in range(npcd):\n",
    "            path = output_directory / f\"{i:04}_pts.ply\"\n",
    "            generate_simulated_pcd(path, ref_pcd_path, noise_std)\n",
    "\n",
    "            delayed_tasks.append(\n",
    "                dask.delayed(generate_simulated_pcd)(path, ref_pcd_path, noise_std)\n",
    "            )\n",
    "\n",
    "        dask.compute(*delayed_tasks)\n",
    "    else:\n",
    "        for i in range(npcd):\n",
    "            path = output_directory / f\"{i:04}_pts.ply\"\n",
    "            generate_simulated_pcd(path, ref_pcd_path, noise_std)\n",
    "\n",
    "\n",
    "generate_random_pcd = False\n",
    "num_pcd = 1000\n",
    "num_points = 100000\n",
    "noise_std = 0.01\n",
    "\n",
    "# Generate Random point cloud as test data\n",
    "if generate_random_pcd:\n",
    "    pcd_dir = Path(\"data/test\")\n",
    "    generate_random_data(pcd_dir, num_pcd, num_points, noise_std)\n",
    "    print(\"Generated random point cloud data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pcd_dir = Path(\"data/test\")\n",
    "# pcd_dir = Path(\"data/rossia\")\n",
    "pcd_dir = Path(\"data/calib\")\n",
    "pcd_ext = \"ply\"\n",
    "cov_ddof = 1\n",
    "\n",
    "# Get pcd list\n",
    "pcd_dir = pcd_dir / \"Monte_Carlo_output\"\n",
    "pcd_list = sorted(list(pcd_dir.glob(f\"*.{pcd_ext}\")))\n",
    "print(f\"Found {len(pcd_list)} pointclouds in {pcd_dir}\")\n",
    "\n",
    "ref_pcd_path = pcd_dir.parent / \"sparse_pts_reference.ply\"\n",
    "if ref_pcd_path.exists():\n",
    "    ref_pcd = load_pcd(ref_pcd_path)\n",
    "    print(f\"Loaded reference pointcloud from {ref_pcd_path}\")\n",
    "else:\n",
    "    ref_pcd = load_pcd(pcd_list[0])\n",
    "    print(\"Reference pointcloud not found, using first pointcloud as reference\")\n",
    "\n",
    "# Build a lazy dask array of all pointclouds\n",
    "stack = lazy_load_pcd_stack(pcd_list)\n",
    "stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and compute the mean and std with dask\n",
    "operations = [\n",
    "    stack.mean(axis=0),\n",
    "    stack.std(axis=0, ddof=cov_ddof),\n",
    "]\n",
    "mean, std = dask.compute(*operations)\n",
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the data and compute the mean and std with numpy\n",
    "# stack = load_pcd_stack(pcd_list)\n",
    "# mean = stack.mean(axis=0)\n",
    "# std = stack.std(axis=0, ddof=cov_ddof)\n",
    "# print(mean) \n",
    "# print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute difference between reference and mean coordinates\n",
    "ref_diff = np.mean(ref_pcd - mean, axis=0)\n",
    "print(f\"{ref_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plot of the mean coordinates of the point colored with the standard deviation of the coordinates\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "point_size = 1\n",
    "scale_fct = 1000\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "plt_x = axes[0].scatter(mean[:, 0], mean[:, 1], s=point_size, c=std[:, 0] * scale_fct)\n",
    "axes[0].set_title(\"Precision X [mm]\")\n",
    "axes[0].set_xlabel(\"X\")\n",
    "axes[0].set_ylabel(\"Y\")\n",
    "fig.colorbar(plt_x)\n",
    "\n",
    "plt_y = axes[1].scatter(mean[:, 0], mean[:, 1], s=point_size, c=std[:, 1] * scale_fct)\n",
    "axes[1].set_title(\"Precision Y [mm]\")\n",
    "axes[1].set_xlabel(\"X\")\n",
    "axes[1].set_ylabel(\"Y\")\n",
    "fig.colorbar(plt_y)\n",
    "\n",
    "plt_z = axes[2].scatter(mean[:, 0], mean[:, 1], s=point_size, c=std[:, 2] * scale_fct)\n",
    "axes[2].set_title(\"Precision Z [mm]\")\n",
    "axes[2].set_xlabel(\"X\")\n",
    "axes[2].set_ylabel(\"Y\")\n",
    "fig.colorbar(plt_z)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thirdparty import transformations as tf\n",
    "\n",
    "T = tf.affine_matrix_from_points(\n",
    "    mean.T, ref_pcd.T, shear=False, scale=True, usesvd=True\n",
    ")\n",
    "scale, _, angles, translation, _ = tf.decompose_matrix(T)\n",
    "scale_percent = scale.mean() - 1\n",
    "angles_deg = np.rad2deg(angles)\n",
    "\n",
    "print(f\"Translation: {translation*1000} mm\")\n",
    "print(f\"Angles: {angles_deg} deg\")\n",
    "print(f\"Scale: {scale_percent:.6}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute covariance matrix with numpy (note that all the pcd are loaded in memory at once here!)\n",
    "def compute_covariance(points):\n",
    "    return np.cov(points, rowvar=False, ddof=cov_ddof)\n",
    "\n",
    "\n",
    "np_stack = np.array(stack)\n",
    "np_cov = [compute_covariance(np_stack[:, i, :]) for i in range(np_stack.shape[1])]\n",
    "np.sqrt(np_cov[0].diagonal()) - std[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute covariance matrix with dask\n",
    "# delayed_dask_cov = [\n",
    "#     dask.delayed(compute_covariance)(stack[:, i, :]) for i in range(stack.shape[1])\n",
    "# ]\n",
    "# dask_cov = da.compute(*delayed_dask_cov)\n",
    "# print(np.sqrt(np.diag(np.array(dask_cov[0]))) - std[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
